{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUBnEIU/ELiWcPcwc5AZhh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prashantmanghnani/ytCommentSentiment/blob/main/ytCommentSentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfFLh1tR35eN",
        "outputId": "72559a34-1fe0-4d3c-8200-00e42923913e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.155.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.25.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.2.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m865.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, pyngrok, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.6 pyngrok-7.2.3 starlette-0.41.3 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn transformers google-api-python-client nltk pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDsTlCAj48Dn",
        "outputId": "3c40b32e-27aa-45ce-e526-2fcc6017cff8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2rTnpXuThEY5L6t5ITP2CyWTSqI_5zDT13Kw6gvyz4HBPrSqn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzyw_URk4Hf-",
        "outputId": "fd1ff4a7-3aeb-4b87-d881-60572b517126"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "import requests\n",
        "from transformers import pipeline\n",
        "from googleapiclient.discovery import build\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Configure CORS to allow requests from all origins (or specify your extension's origin)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Or specify your Chrome extension's URL, e.g. [\"chrome-extension://your-extension-id\"]\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Initialize Hugging Face sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# YouTube API setup\n",
        "API_KEY = \"AIzaSyANE1eR9kDd0Ng04w5rf3l5635vkTyyqa8\"  # Replace with your API key\n",
        "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
        "YOUTUBE_API_VERSION = \"v3\"\n",
        "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)\n",
        "\n",
        "# Helper function to clean the comments\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Helper function to fetch comments\n",
        "def fetch_comments(video_id, max_results=100):\n",
        "    comments = []\n",
        "    try:\n",
        "        response = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            maxResults=max_results,\n",
        "            textFormat=\"plainText\"\n",
        "        ).execute()\n",
        "\n",
        "        for item in response.get(\"items\", []):\n",
        "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "            comments.append(clean_text(comment))\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching comments: {e}\")\n",
        "    return comments\n",
        "\n",
        "# Helper function to analyze sentiments\n",
        "def analyze_comments(comments):\n",
        "    total_sentiments = {\"positive\": 0, \"negative\": 0, \"neutral\": 0}\n",
        "    sentiment_scores = []\n",
        "    sentiments = []\n",
        "\n",
        "    for comment in comments:\n",
        "        result = sentiment_analyzer(comment)[0]\n",
        "        sentiments.append(result)\n",
        "        sentiment_scores.append(result['score'])\n",
        "\n",
        "        if result[\"label\"] == \"POSITIVE\":\n",
        "            total_sentiments[\"positive\"] += 1\n",
        "        elif result[\"label\"] == \"NEGATIVE\":\n",
        "            total_sentiments[\"negative\"] += 1\n",
        "        else:\n",
        "            total_sentiments[\"neutral\"] += 1\n",
        "\n",
        "    avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0\n",
        "    return sentiments, total_sentiments, avg_sentiment_score\n",
        "\n",
        "# Helper function to calculate metrics\n",
        "def calculate_metrics(comments):\n",
        "    unique_comments = set(comments)\n",
        "    total_length = sum(len(comment) for comment in comments)\n",
        "    avg_length = total_length / len(comments) if comments else 0\n",
        "    return {\n",
        "        \"total_comments\": len(comments),\n",
        "        \"unique_comments\": len(unique_comments),\n",
        "        \"avg_length\": avg_length\n",
        "    }\n",
        "\n",
        "# API endpoint to analyze video comments\n",
        "@app.get(\"/analyze/{video_id}\")\n",
        "async def analyze_video(video_id: str):\n",
        "    comments = fetch_comments(video_id, max_results=100)\n",
        "\n",
        "    if not comments:\n",
        "        return {\"message\": \"No comments found or failed to fetch comments.\"}\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = calculate_metrics(comments)\n",
        "\n",
        "    # Sentiment analysis\n",
        "    sentiments, sentiment_counts, avg_sentiment_score = analyze_comments(comments)\n",
        "\n",
        "    # Return analysis results\n",
        "    return {\n",
        "        \"message\": \"Analysis complete\",\n",
        "        \"video_id\": video_id,\n",
        "        \"total_comments\": metrics[\"total_comments\"],\n",
        "        \"unique_comments\": metrics[\"unique_comments\"],\n",
        "        \"avg_length\": metrics[\"avg_length\"],\n",
        "        \"sentiment_analysis\": sentiment_counts,\n",
        "        \"avg_sentiment_score\": avg_sentiment_score,\n",
        "    }\n",
        "\n",
        "# Open a ngrok tunnel to the FastAPI server\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"FastAPI server is running at: {public_url}\")\n",
        "\n",
        "# Run the FastAPI app with Uvicorn\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RenDm0-Y3-dH",
        "outputId": "b3db28da-5699-48cd-8885-32ad5e10b9a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FastAPI server is running at: NgrokTunnel: \"https://84d4-34-58-126-111.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [1481]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET / HTTP/1.1\" 404 Not Found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (811 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/D5d5xinZI3E HTTP/1.1\" 500 Internal Server Error\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 715, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 735, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 73, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 301, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-8-bfc313824c88>\", line 107, in analyze_video\n",
            "    sentiments, sentiment_counts, avg_sentiment_score = analyze_comments(comments)\n",
            "                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-8-bfc313824c88>\", line 70, in analyze_comments\n",
            "    result = sentiment_analyzer(comment)[0]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py\", line 159, in __call__\n",
            "    result = super().__call__(*inputs, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 1301, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 1308, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 1208, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py\", line 190, in _forward\n",
            "    return self.model(**model_inputs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 978, in forward\n",
            "    distilbert_output = self.distilbert(\n",
            "                        ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 785, in forward\n",
            "    embeddings = self.embeddings(input_ids, inputs_embeds)  # (bs, seq_length, dim)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 131, in forward\n",
            "    embeddings = input_embeds + position_embeddings  # (bs, max_seq_length, dim)\n",
            "                 ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
            "RuntimeError: The size of tensor a (811) must match the size of tensor b (512) at non-singleton dimension 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/D5d5xinZI3E HTTP/1.1\" 500 Internal Server Error\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 715, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 735, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 73, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 301, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-8-bfc313824c88>\", line 107, in analyze_video\n",
            "    sentiments, sentiment_counts, avg_sentiment_score = analyze_comments(comments)\n",
            "                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-8-bfc313824c88>\", line 70, in analyze_comments\n",
            "    result = sentiment_analyzer(comment)[0]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py\", line 159, in __call__\n",
            "    result = super().__call__(*inputs, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 1301, in __call__\n",
            "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 1308, in run_single\n",
            "    model_outputs = self.forward(model_inputs, **forward_params)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\", line 1208, in forward\n",
            "    model_outputs = self._forward(model_inputs, **forward_params)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py\", line 190, in _forward\n",
            "    return self.model(**model_inputs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 978, in forward\n",
            "    distilbert_output = self.distilbert(\n",
            "                        ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 785, in forward\n",
            "    embeddings = self.embeddings(input_ids, inputs_embeds)  # (bs, seq_length, dim)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 131, in forward\n",
            "    embeddings = input_embeds + position_embeddings  # (bs, max_seq_length, dim)\n",
            "                 ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
            "RuntimeError: The size of tensor a (811) must match the size of tensor b (512) at non-singleton dimension 1\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [1481]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import requests\n",
        "from transformers import pipeline\n",
        "from googleapiclient.discovery import build\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Configure CORS to allow requests from all origins (or specify your extension's origin)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Or specify your Chrome extension's URL, e.g. [\"chrome-extension://your-extension-id\"]\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Initialize Hugging Face sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# YouTube API setup\n",
        "API_KEY = \"AIzaSyANE1eR9kDd0Ng04w5rf3l5635vkTyyqa8\"  # Replace with your API key\n",
        "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
        "YOUTUBE_API_VERSION = \"v3\"\n",
        "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)\n",
        "\n",
        "# Helper function to clean the comments\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Helper function to fetch comments\n",
        "def fetch_comments(video_id, max_results=100):\n",
        "    comments = []\n",
        "    try:\n",
        "        response = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            maxResults=max_results,\n",
        "            textFormat=\"plainText\"\n",
        "        ).execute()\n",
        "\n",
        "        for item in response.get(\"items\", []):\n",
        "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "            comments.append(clean_text(comment))\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching comments: {e}\")\n",
        "    return comments\n",
        "\n",
        "\n",
        "# Helper function to analyze sentiments\n",
        "def analyze_comments(comments):\n",
        "    total_sentiments = {\"positive\": 0, \"negative\": 0, \"neutral\": 0}\n",
        "    sentiment_scores = []\n",
        "    sentiments = []\n",
        "\n",
        "    for comment in comments:\n",
        "        # Truncate comment to a maximum length of 512 characters\n",
        "        comment = comment[:512]\n",
        "\n",
        "        result = sentiment_analyzer(comment)[0]\n",
        "        sentiments.append(result)\n",
        "        sentiment_scores.append(result['score'])\n",
        "\n",
        "        if result[\"label\"] == \"POSITIVE\":\n",
        "            total_sentiments[\"positive\"] += 1\n",
        "        elif result[\"label\"] == \"NEGATIVE\":\n",
        "            total_sentiments[\"negative\"] += 1\n",
        "        else:\n",
        "            total_sentiments[\"neutral\"] += 1\n",
        "\n",
        "    avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0\n",
        "    return sentiments, total_sentiments, avg_sentiment_score\n",
        "\n",
        "\n",
        "# Helper function to calculate metrics\n",
        "def calculate_metrics(comments):\n",
        "    unique_comments = set(comments)\n",
        "    total_length = sum(len(comment) for comment in comments)\n",
        "    avg_length = total_length / len(comments) if comments else 0\n",
        "    return {\n",
        "        \"total_comments\": len(comments),\n",
        "        \"unique_comments\": len(unique_comments),\n",
        "        \"avg_length\": avg_length\n",
        "    }\n",
        "\n",
        "# API endpoint to analyze video comments\n",
        "@app.get(\"/analyze/{video_id}\")\n",
        "async def analyze_video(video_id: str):\n",
        "    comments = fetch_comments(video_id, max_results=100)\n",
        "\n",
        "    if not comments:\n",
        "        return {\"message\": \"No comments found or failed to fetch comments.\"}\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = calculate_metrics(comments)\n",
        "\n",
        "    # Sentiment analysis\n",
        "    sentiments, sentiment_counts, avg_sentiment_score = analyze_comments(comments)\n",
        "\n",
        "    # Return analysis results\n",
        "    return {\n",
        "        \"message\": \"Analysis complete\",\n",
        "        \"video_id\": video_id,\n",
        "        \"total_comments\": metrics[\"total_comments\"],\n",
        "        \"unique_comments\": metrics[\"unique_comments\"],\n",
        "        \"avg_length\": metrics[\"avg_length\"],\n",
        "        \"sentiment_analysis\": sentiment_counts,\n",
        "        \"avg_sentiment_score\": avg_sentiment_score,\n",
        "    }\n",
        "\n",
        "# Open a ngrok tunnel to the FastAPI server\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"FastAPI server is running at: {public_url}\")\n",
        "\n",
        "# Run the FastAPI app with Uvicorn\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBIbl1dG6Kys",
        "outputId": "91427990-d791-47c8-d4fd-642b8fc544b9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI server is running at: NgrokTunnel: \"https://98c0-34-58-126-111.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [1481]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /MRLyREkZles HTTP/1.1\" 404 Not Found\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/MRLyREkZles HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-01-20T10:04:10+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8000-49925acc-84b3-49ca-ab44-2406090e38c1 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-01-20T10:04:10+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8000-49925acc-84b3-49ca-ab44-2406090e38c1 err=\"failed to start tunnel: session closed\"\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [1481]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import requests\n",
        "from transformers import pipeline\n",
        "from googleapiclient.discovery import build\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Configure CORS to allow requests from all origins (or specify your extension's origin)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Or specify your Chrome extension's URL, e.g. [\"chrome-extension://your-extension-id\"]\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Initialize Hugging Face sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# YouTube API setup\n",
        "API_KEY = \"AIzaSyANE1eR9kDd0Ng04w5rf3l5635vkTyyqa8\"  # Replace with your API key\n",
        "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
        "YOUTUBE_API_VERSION = \"v3\"\n",
        "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Helper function to clean the comments\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Helper function to fetch comments from YouTube API\n",
        "def fetch_comments(video_id, max_results=100):\n",
        "    comments = []\n",
        "    try:\n",
        "        response = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            maxResults=max_results,\n",
        "            textFormat=\"plainText\"\n",
        "        ).execute()\n",
        "\n",
        "        for item in response.get(\"items\", []):\n",
        "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "            comments.append(clean_text(comment))\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching comments: {e}\")\n",
        "    return comments\n",
        "\n",
        "# Helper function to analyze sentiments\n",
        "def analyze_comments(comments):\n",
        "    total_sentiments = {\"very_positive\": 0, \"positive\": 0, \"neutral\": 0, \"negative\": 0, \"very_negative\": 0}\n",
        "    sentiment_scores = []\n",
        "    sentiments = []\n",
        "\n",
        "    for comment in comments:\n",
        "        result = sentiment_analyzer(comment)[0]\n",
        "        sentiments.append(result)\n",
        "        sentiment_scores.append(result['score'])\n",
        "\n",
        "        # Classify sentiments based on score thresholds\n",
        "        if result[\"label\"] == \"POSITIVE\":\n",
        "            if result[\"score\"] > 0.90:\n",
        "                total_sentiments[\"very_positive\"] += 1\n",
        "            else:\n",
        "                total_sentiments[\"positive\"] += 1\n",
        "        elif result[\"label\"] == \"NEGATIVE\":\n",
        "            if result[\"score\"] > 0.90:\n",
        "                total_sentiments[\"very_negative\"] += 1\n",
        "            else:\n",
        "                total_sentiments[\"negative\"] += 1\n",
        "        else:\n",
        "            total_sentiments[\"neutral\"] += 1\n",
        "\n",
        "    avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0\n",
        "    return sentiments, total_sentiments, avg_sentiment_score\n",
        "\n",
        "# Helper function to calculate metrics\n",
        "def calculate_metrics(comments):\n",
        "    unique_comments = set(comments)\n",
        "    total_length = sum(len(comment) for comment in comments)\n",
        "    avg_length = total_length / len(comments) if comments else 0\n",
        "    return {\n",
        "        \"total_comments\": len(comments),\n",
        "        \"unique_comments\": len(unique_comments),\n",
        "        \"avg_length\": avg_length\n",
        "    }\n",
        "\n",
        "# API endpoint to analyze video comments\n",
        "@app.get(\"/analyze/{video_id}\")\n",
        "async def analyze_video(video_id: str):\n",
        "    comments = fetch_comments(video_id, max_results=100)\n",
        "\n",
        "    if not comments:\n",
        "        return {\"message\": \"No comments found or failed to fetch comments.\"}\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = calculate_metrics(comments)\n",
        "\n",
        "    # Sentiment analysis\n",
        "    sentiments, sentiment_counts, avg_sentiment_score = analyze_comments(comments)\n",
        "\n",
        "    # Return analysis results\n",
        "    return {\n",
        "        \"message\": \"Analysis complete\",\n",
        "        \"video_id\": video_id,\n",
        "        \"total_comments\": metrics[\"total_comments\"],\n",
        "        \"unique_comments\": metrics[\"unique_comments\"],\n",
        "        \"avg_length\": metrics[\"avg_length\"],\n",
        "        \"sentiment_analysis\": sentiment_counts,\n",
        "        \"avg_sentiment_score\": avg_sentiment_score,\n",
        "    }\n",
        "\n",
        "# Open a ngrok tunnel to the FastAPI server\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"FastAPI server is running at: {public_url}\")\n",
        "\n",
        "# Run the FastAPI app with Uvicorn\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWY_fU_kJ3SO",
        "outputId": "50c47b0a-850f-459e-a1c1-eb2895cfd53b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI server is running at: NgrokTunnel: \"https://47bc-34-58-126-111.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [1481]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/5eQu6MFGoG8 HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/5eQu6MFGoG8 HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/5eQu6MFGoG8 HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/5eQu6MFGoG8 HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/5eQu6MFGoG8 HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/5eQu6MFGoG8 HTTP/1.1\" 200 OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import requests\n",
        "from transformers import pipeline\n",
        "from googleapiclient.discovery import build\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import base64\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import asyncio\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Apply nest_asyncio for running the event loop in Jupyter environments (like Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Configure CORS to allow requests from all origins (or specify your Chrome extension's origin)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Or specify your Chrome extension's URL, e.g. [\"chrome-extension://your-extension-id\"]\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Initialize Hugging Face sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# YouTube API setup\n",
        "API_KEY = \"AIzaSyANE1eR9kDd0Ng04w5rf3l5635vkTyyqa8\"  # Replace with your API key\n",
        "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
        "YOUTUBE_API_VERSION = \"v3\"\n",
        "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)\n",
        "\n",
        "# Helper function to clean the comments\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Helper function to fetch comments from YouTube API\n",
        "def fetch_comments(video_id, max_results=100):\n",
        "    comments = []\n",
        "    try:\n",
        "        response = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            maxResults=max_results,\n",
        "            textFormat=\"plainText\"\n",
        "        ).execute()\n",
        "\n",
        "        for item in response.get(\"items\", []):\n",
        "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "            comments.append(clean_text(comment))\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching comments: {e}\")\n",
        "    return comments\n",
        "\n",
        "# Helper function to analyze sentiments\n",
        "def analyze_comments(comments):\n",
        "    total_sentiments = {\"very_positive\": 0, \"positive\": 0, \"neutral\": 0, \"negative\": 0, \"very_negative\": 0}\n",
        "    sentiment_scores = []\n",
        "    sentiments = []\n",
        "\n",
        "    for comment in comments:\n",
        "        result = sentiment_analyzer(comment)[0]\n",
        "        sentiments.append(result)\n",
        "        sentiment_scores.append(result['score'])\n",
        "\n",
        "        # Classify sentiments based on score thresholds\n",
        "        if result[\"label\"] == \"POSITIVE\":\n",
        "            if result[\"score\"] > 0.90:\n",
        "                total_sentiments[\"very_positive\"] += 1\n",
        "            else:\n",
        "                total_sentiments[\"positive\"] += 1\n",
        "        elif result[\"label\"] == \"NEGATIVE\":\n",
        "            if result[\"score\"] > 0.90:\n",
        "                total_sentiments[\"very_negative\"] += 1\n",
        "            else:\n",
        "                total_sentiments[\"negative\"] += 1\n",
        "        else:\n",
        "            total_sentiments[\"neutral\"] += 1\n",
        "\n",
        "    avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0\n",
        "    return sentiments, total_sentiments, avg_sentiment_score\n",
        "\n",
        "# Helper function to calculate comment metrics\n",
        "def calculate_metrics(comments):\n",
        "    unique_comments = set(comments)\n",
        "    total_length = sum(len(comment) for comment in comments)\n",
        "    avg_length = total_length / len(comments) if comments else 0\n",
        "    return {\n",
        "        \"total_comments\": len(comments),\n",
        "        \"unique_comments\": len(unique_comments),\n",
        "        \"avg_length\": avg_length\n",
        "    }\n",
        "\n",
        "# Helper function to clean and process comments for word cloud generation\n",
        "def clean_and_process_comments(comments):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    all_words = []\n",
        "\n",
        "    for comment in comments:\n",
        "        words = word_tokenize(comment)\n",
        "        filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "        all_words.extend(filtered_words)\n",
        "\n",
        "    return all_words\n",
        "\n",
        "# Helper function to generate word cloud\n",
        "def generate_word_cloud(comments):\n",
        "    # Clean and process comments to get most common words\n",
        "    all_words = clean_and_process_comments(comments)\n",
        "\n",
        "    # Count word frequency\n",
        "    word_freq = Counter(all_words)\n",
        "\n",
        "    # Create a WordCloud object\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "\n",
        "    # Convert wordcloud to base64 string\n",
        "    img_buffer = io.BytesIO()\n",
        "    wordcloud.to_image().save(img_buffer, format=\"PNG\")\n",
        "    img_base64 = base64.b64encode(img_buffer.getvalue()).decode('utf-8')\n",
        "\n",
        "    return img_base64\n",
        "\n",
        "# API endpoint to analyze video comments\n",
        "@app.get(\"/analyze/{video_id}\")\n",
        "async def analyze_video(video_id: str):\n",
        "    comments = fetch_comments(video_id, max_results=100)\n",
        "\n",
        "    if not comments:\n",
        "        return {\"message\": \"No comments found or failed to fetch comments.\"}\n",
        "\n",
        "    # Generate word cloud\n",
        "    wordcloud_base64 = generate_word_cloud(comments)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = calculate_metrics(comments)\n",
        "\n",
        "    # Sentiment analysis\n",
        "    sentiments, sentiment_counts, avg_sentiment_score = analyze_comments(comments)\n",
        "\n",
        "    # Return analysis results along with word cloud base64 string\n",
        "    return {\n",
        "        \"message\": \"Analysis complete\",\n",
        "        \"video_id\": video_id,\n",
        "        \"total_comments\": metrics[\"total_comments\"],\n",
        "        \"unique_comments\": metrics[\"unique_comments\"],\n",
        "        \"avg_length\": metrics[\"avg_length\"],\n",
        "        \"sentiment_analysis\": sentiment_counts,\n",
        "        \"avg_sentiment_score\": avg_sentiment_score,\n",
        "        \"wordcloud\": wordcloud_base64,  # This will be used to display the word cloud in the extension\n",
        "        \"sentiment_score_description\": get_sentiment_description(avg_sentiment_score)  # Human-readable sentiment explanation\n",
        "    }\n",
        "\n",
        "# Helper function to provide a human-readable sentiment score description\n",
        "def get_sentiment_description(avg_sentiment_score):\n",
        "    if avg_sentiment_score >= 0.90:\n",
        "        return \"Very Positive\"\n",
        "    elif avg_sentiment_score >= 0.75:\n",
        "        return \"Positive\"\n",
        "    elif avg_sentiment_score >= 0.25:\n",
        "        return \"Neutral\"\n",
        "    elif avg_sentiment_score >= 0.10:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Very Negative\"\n",
        "\n",
        "# Open a ngrok tunnel to the FastAPI server\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"FastAPI server is running at: {public_url}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyp6PYZ0HcZI",
        "outputId": "b06ea0bf-68ce-4637-98b6-af363c25e6a4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-94' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 579, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI server is running at: NgrokTunnel: \"https://47a3-34-58-126-111.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import requests\n",
        "from transformers import pipeline\n",
        "from googleapiclient.discovery import build\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import base64\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import asyncio\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Apply nest_asyncio for running the event loop in Jupyter environments (like Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Configure CORS to allow requests from all origins (or specify your Chrome extension's origin)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Or specify your Chrome extension's URL, e.g. [\"chrome-extension://your-extension-id\"]\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Initialize Hugging Face sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# YouTube API setup\n",
        "API_KEY = \"AIzaSyANE1eR9kDd0Ng04w5rf3l5635vkTyyqa8\"  # Replace with your API key\n",
        "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
        "YOUTUBE_API_VERSION = \"v3\"\n",
        "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)\n",
        "\n",
        "# Helper function to clean the comments\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Helper function to fetch comments from YouTube API\n",
        "def fetch_comments(video_id, max_results=100):\n",
        "    comments = []\n",
        "    try:\n",
        "        response = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            maxResults=max_results,\n",
        "            textFormat=\"plainText\"\n",
        "        ).execute()\n",
        "\n",
        "        for item in response.get(\"items\", []):\n",
        "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "            comments.append(clean_text(comment))\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching comments: {e}\")\n",
        "    return comments\n",
        "\n",
        "# Helper function to analyze sentiments\n",
        "def analyze_comments(comments):\n",
        "    total_sentiments = {\"very_positive\": 0, \"positive\": 0, \"neutral\": 0, \"negative\": 0, \"very_negative\": 0}\n",
        "    sentiment_scores = []\n",
        "    sentiments = []\n",
        "\n",
        "    for comment in comments:\n",
        "        result = sentiment_analyzer(comment)[0]\n",
        "        sentiments.append(result)\n",
        "        sentiment_scores.append(result['score'])\n",
        "\n",
        "        # Classify sentiments based on score thresholds\n",
        "        if result[\"label\"] == \"POSITIVE\":\n",
        "            if result[\"score\"] > 0.90:\n",
        "                total_sentiments[\"very_positive\"] += 1\n",
        "            else:\n",
        "                total_sentiments[\"positive\"] += 1\n",
        "        elif result[\"label\"] == \"NEGATIVE\":\n",
        "            if result[\"score\"] > 0.90:\n",
        "                total_sentiments[\"very_negative\"] += 1\n",
        "            else:\n",
        "                total_sentiments[\"negative\"] += 1\n",
        "        else:\n",
        "            total_sentiments[\"neutral\"] += 1\n",
        "\n",
        "    avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0\n",
        "    return sentiments, total_sentiments, avg_sentiment_score\n",
        "\n",
        "# Helper function to calculate comment metrics\n",
        "def calculate_metrics(comments):\n",
        "    unique_comments = set(comments)\n",
        "    total_length = sum(len(comment) for comment in comments)\n",
        "    avg_length = total_length / len(comments) if comments else 0\n",
        "    return {\n",
        "        \"total_comments\": len(comments),\n",
        "        \"unique_comments\": len(unique_comments),\n",
        "        \"avg_length\": avg_length\n",
        "    }\n",
        "\n",
        "# Helper function to clean and process comments for word cloud generation\n",
        "def clean_and_process_comments(comments):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    all_words = []\n",
        "\n",
        "    for comment in comments:\n",
        "        words = word_tokenize(comment)\n",
        "        filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "        all_words.extend(filtered_words)\n",
        "\n",
        "    return all_words\n",
        "\n",
        "# Helper function to generate word cloud\n",
        "def generate_word_cloud(comments):\n",
        "    # Clean and process comments to get most common words\n",
        "    all_words = clean_and_process_comments(comments)\n",
        "\n",
        "    # Count word frequency\n",
        "    word_freq = Counter(all_words)\n",
        "\n",
        "    # Create a WordCloud object\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "\n",
        "    # Convert wordcloud to base64 string\n",
        "    img_buffer = io.BytesIO()\n",
        "    wordcloud.to_image().save(img_buffer, format=\"PNG\")\n",
        "    img_base64 = base64.b64encode(img_buffer.getvalue()).decode('utf-8')\n",
        "\n",
        "    return img_base64\n",
        "\n",
        "# API endpoint to analyze video comments\n",
        "@app.get(\"/analyze/{video_id}\")\n",
        "async def analyze_video(video_id: str):\n",
        "    comments = fetch_comments(video_id, max_results=100)\n",
        "\n",
        "    if not comments:\n",
        "        return {\"message\": \"No comments found or failed to fetch comments.\"}\n",
        "\n",
        "    # Generate word cloud\n",
        "    wordcloud_base64 = generate_word_cloud(comments)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = calculate_metrics(comments)\n",
        "\n",
        "    # Sentiment analysis\n",
        "    sentiments, sentiment_counts, avg_sentiment_score = analyze_comments(comments)\n",
        "\n",
        "    # Return analysis results along with word cloud base64 string\n",
        "    return {\n",
        "        \"message\": \"Analysis complete\",\n",
        "        \"sentiment_score_description\": get_sentiment_description(avg_sentiment_score),  # Human-readable sentiment explanation\n",
        "        \"video_id\": video_id,\n",
        "        \"total_comments\": metrics[\"total_comments\"],\n",
        "        \"unique_comments\": metrics[\"unique_comments\"],\n",
        "        \"avg_length\": metrics[\"avg_length\"],\n",
        "        \"sentiment_analysis\": sentiment_counts,\n",
        "        \"avg_sentiment_score\": avg_sentiment_score,\n",
        "        \"wordcloud\": wordcloud_base64  # This will be used to display the word cloud in the extension\n",
        "    }\n",
        "\n",
        "# Helper function to provide a human-readable sentiment score description\n",
        "def get_sentiment_description(avg_sentiment_score):\n",
        "    if avg_sentiment_score >= 0.90:\n",
        "        return \"Very Positive\"\n",
        "    elif avg_sentiment_score >= 0.75:\n",
        "        return \"Positive\"\n",
        "    elif avg_sentiment_score >= 0.25:\n",
        "        return \"Neutral\"\n",
        "    elif avg_sentiment_score >= 0.10:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Very Negative\"\n",
        "\n",
        "# Open a ngrok tunnel to the FastAPI server\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"FastAPI server is running at: {public_url}\")\n",
        "\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Hello World\"}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Graceful shutdown handling\n",
        "    try:\n",
        "        uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Server shutdown gracefully.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC4g2XTrT6we",
        "outputId": "fa300462-9bbd-447a-e80f-ea814151a21d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI server is running at: NgrokTunnel: \"https://f95e-34-58-126-111.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [1481]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/UKInLPwr7wk HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/UKInLPwr7wk HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/UKInLPwr7wk HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/UKInLPwr7wk HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/UKInLPwr7wk HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/UKInLPwr7wk HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/UKInLPwr7wk HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/UKInLPwr7wk HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/UKInLPwr7wk HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/UKInLPwr7wk HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/UKInLPwr7wk HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/G0JKdFjWkLA HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/UKInLPwr7wk HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/UKInLPwr7wk HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/G0JKdFjWkLA HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/G0JKdFjWkLA HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/G0JKdFjWkLA HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/G0JKdFjWkLA HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/G0JKdFjWkLA HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/G0JKdFjWkLA HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/G0JKdFjWkLA HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/G0JKdFjWkLA HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/G0JKdFjWkLA HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/5eQu6MFGoG8 HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c7b:1136:ace1:e53b:d65c:4d17:0 - \"GET /analyze/5eQu6MFGoG8 HTTP/1.1\" 200 OK\n",
            "INFO:     51.8.155.61:0 - \"GET /robots.txt HTTP/1.1\" 404 Not Found\n",
            "INFO:     51.8.155.59:0 - \"GET /analyze/5eQu6MFGoG8 HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [1481]\n"
          ]
        }
      ]
    }
  ]
}